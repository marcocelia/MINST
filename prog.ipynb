{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('envpytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "28a46be15962e7318714efe17d88605277333355411ea063dbfd39bb67b66883"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "tf_noNorm=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(dataset_path, train=True, download=True , transform=tf_noNorm)\n",
    "test_set = torchvision.datasets.FashionMNIST(dataset_path, train=False, download=True , transform=tf_noNorm)"
   ]
  },
  {
   "source": [
    "### Categorie\n",
    "```\n",
    "0 T-shirt/top\n",
    "1 Trouser\n",
    "2 Pullover\n",
    "3 Dress\n",
    "4 Coat\n",
    "5 Sandal\n",
    "6 Shirt\n",
    "7 Sneaker\n",
    "8 Bag\n",
    "9 Ankle boot\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size_train)\n",
    "\n",
    "batch_size_test = 1000\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "source": [
    "## SHOW SOME DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_target) = next(examples)\n",
    "\n",
    "f, axs = plt.subplots(3,3)\n",
    "plt.tight_layout()\n",
    "k = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axs[i][j].imshow(example_data[k][0], cmap='gray', interpolation='none')\n",
    "        axs[i][j].set_title(\"Ground Truth: {}\".format(example_target[k]))\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## NET ARCH\n",
    "\n",
    "input\n",
    "28Ã—28 \n",
    "\n",
    "output\n",
    "10 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # conv parameters\n",
    "        self.ks1 = 5\n",
    "        self.nk1 = 10\n",
    "        self.ks2 = 5\n",
    "        self.nk2 = 20\n",
    "        self.out_conv1 = (int)((28 - self.ks1 + 1)/2)\n",
    "        self.out_conv2 = (int)((self.out_conv1 - self.ks2 + 1)/2)\n",
    "        self.n_unit1 = self.nk2*self.out_conv2**2\n",
    "        self.n_unit2 = 50\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, self.nk1, kernel_size=self.ks1)\n",
    "        self.conv2 = nn.Conv2d(self.nk1, self.nk2, kernel_size=self.ks2)\n",
    "        self.fc1 = nn.Linear(self.n_unit1, self.n_unit2)\n",
    "        self.fc2 = nn.Linear(self.n_unit2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, self.n_unit1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(*, ld, net, opt, epochs):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        for batch_idx, (data, target) in enumerate(ld):\n",
    "            opt.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                num_campioni_elaborati = batch_idx * len(data)\n",
    "                num_campioni_totali = len(ld.dataset)\n",
    "                print('Train Epoch: {} [{}/{}  ({:.0f}%)]  \\tLoss: {:.6f}'.format(\n",
    "                    epoch, num_campioni_elaborati, num_campioni_totali , \n",
    "                    100. * num_campioni_elaborati / num_campioni_totali, \n",
    "                    loss.item()\n",
    "                    )\n",
    "                )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(*,tls=None, net, ld):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in ld:\n",
    "            output = net(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            all_preds += pred\n",
    "            all_targets += target\n",
    "\n",
    "    test_loss /= len(ld.dataset)\n",
    "    correct = accuracy_score(all_targets, all_preds, normalize=False)\n",
    "    accuracy = 100. * correct / len(ld.dataset)\n",
    "    precision = 100*precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = 100*recall_score(all_targets, all_preds, average='macro')\n",
    "    return np.array([test_loss, accuracy, precision, recall, correct])"
   ]
  },
  {
   "source": [
    "## 10-FOLD CROSS VALIDATION ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv(*, train, valid, model, dataset, bs, epochs, k_fold=5):\n",
    "\n",
    "    train_score = pd.Series()\n",
    "    val_score = pd.Series()\n",
    "\n",
    "    total_size = len(dataset)\n",
    "    seg = int(total_size/k_fold)\n",
    "    # tr:train,val:valid; r:right,l:left;  eg: tr_rr: right index of right side train subset\n",
    "    # index: [tr_ll,tr_lr],[val_l,val_r],[tr_rl,tr_rr]\n",
    "    for i in range(k_fold):\n",
    "        tr_ll = 0\n",
    "        tr_lr = i * seg\n",
    "        val_l = tr_lr\n",
    "        val_r = val_l + seg\n",
    "        tr_rl = val_r\n",
    "        tr_rr = total_size\n",
    "        print(f\"-------  K-FOLD CV - {i}  -------\")\n",
    "\n",
    "        train_indices = list(range(tr_ll,tr_lr)) + list(range(tr_rl,tr_rr))\n",
    "        val_indices = list(range(val_l,val_r))\n",
    "\n",
    "        train_set = torch.utils.data.dataset.Subset(dataset,train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(dataset,val_indices)\n",
    "\n",
    "        tr_ld = torch.utils.data.DataLoader(train_set, batch_size=bs, shuffle=True, num_workers=4)\n",
    "        val_ld = torch.utils.data.DataLoader(val_set, batch_size=int(len(val_set)/10), shuffle=True, num_workers=4)\n",
    "\n",
    "        network = model()\n",
    "        optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.5 )\n",
    "\n",
    "        print(f\"Start Training for indices: [{tr_ll},{tr_lr}),[{tr_rl},{tr_rr})\")\n",
    "        train(net=network, opt=optimizer, ld=tr_ld, epochs=epochs)\n",
    "        \n",
    "        print(f\"Start Validation for indices: [{val_l},{val_r})\")\n",
    "        val_acc = valid(net=network, ld=val_ld)\n",
    "        \n",
    "        print(f\"Avg.Loss = {val_acc[0]:.2f}, Accuracy = {val_acc[1]:.2f}%, Precision = {val_acc[2]:.2f}%, Recall = {val_acc[3]:.2f}%, Correct = {int(val_acc[4])}\")\n",
    "        val_score.at[i] = val_acc\n",
    "        \n",
    "        print()\n",
    "    return val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "kfold = 2\n",
    "score = kfold_cv(train=train, valid=test, model=Net,dataset=train_set, bs=64, epochs=1, k_fold=kfold)\n",
    "score = np.array(score)\n",
    "avg = np.mean(score, axis=0)\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Avg.Loss = {avg[0]:.2f}, Avg.Accuracy = {avg[1]:.2f}%, Avg.Precision = {avg[2]:.2f}%, Avg.Recall = {avg[3]:.2f}%,  Avg.Correct = {avg[4]:.2f}/{int(len(train_set)/kfold)}\")\n"
   ]
  },
  {
   "source": [
    "## Train and Validation ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.5 )\n",
    "\n",
    "train(net=network, opt=optimizer, ld=train_loader, epochs=2)\n",
    "\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    output = network(x)\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    all_preds += pred\n",
    "    all_targets += target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")\n",
    "print(f\"Recall = {100*recall_score(all_targets, all_preds, average='macro'):.2f}%\")\n",
    "print(f\"Precision = {100*precision_score(all_targets, all_preds, average='macro'):.2f}%\")\n",
    "print(f\"Accuracy = {100*accuracy_score(all_targets, all_preds):.2f}%\")"
   ]
  }
 ]
}